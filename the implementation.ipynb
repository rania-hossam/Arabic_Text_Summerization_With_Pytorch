{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk0CbC6SckBj",
        "outputId": "501dd989-b4d2-425a-80a8-8bec003e7293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.3-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 multiprocess-0.70.14 xxhash-3.2.0\n",
            "Collecting arabert\n",
            "  Downloading arabert-1.0.1-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyArabic (from arabert)\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting farasapy (from arabert)\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Collecting emoji==1.4.2 (from arabert)\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.65.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186458 sha256=6baf58647f7564b2e48a4fb831f5fa806970c7e5092153bae0a7b33edfff9d14\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, PyArabic, farasapy, arabert\n",
            "Successfully installed PyArabic-0.6.15 arabert-1.0.1 emoji-1.4.2 farasapy-0.0.14\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting arabicnlp\n",
            "  Downloading arabicnlp-0.1.7-py3-none-any.whl (14.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from arabicnlp) (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from arabicnlp) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.4.10)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->arabicnlp) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->arabicnlp) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->arabicnlp) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (3.2.2)\n",
            "Installing collected packages: arabicnlp\n",
            "Successfully installed arabicnlp-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install arabert\n",
        "!pip install sentencepiece\n",
        "!pip install  arabicnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lRRnysD9aCC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ID_ZWMWcrXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76eb8147-f418-46b8-c931-3a77ac79a6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def _remove_diacritics(text):\n",
        "    text = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n",
        "    return  re.sub(r\"[ًًٌٍَُِّْ]\", \"\", text)\n",
        "\n",
        "def _remove_extra_spaces(text):\n",
        "    return re.sub(\" +\", \" \", text)\n",
        "\n",
        "def _add_spaces_to_all_special_chars(text):\n",
        "    return re.sub(r\"(?<=\\w)([؟.,،])\", r\" \\1\", text)\n",
        "\n",
        "def _remove_repeated_chars(text):\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\\1\", text)\n",
        "\n",
        "def _remove_qoutes(text):\n",
        "    #text = re.sub(r'\"(.*?)\"',\"\",text) # remove the text between two double qoutes\n",
        "    #text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text) # remove the urls\n",
        "    text = re.sub(r\"[0-9]\",\" \",text)\n",
        "    return re.sub(r\"\\[(.*?)\\]\", \" \" , text) # remove the text between two brackets\n",
        "def _remove_puncs(text):\n",
        "    puncs = string.punctuation.replace(\",\",\"\").replace(\"،\",\"\").replace(\"?\",\"؟\")\n",
        "    puncs = puncs.replace(\")\",\"\").replace(\"(\",\"\")\n",
        "    text = text.translate(str.maketrans(' ', ' ', puncs))\n",
        "    text = text.replace(\"•\",\"\")\n",
        "    text = text.replace(\")\",\" \").replace(\"(\",\" \").replace(\":\",\"\")\n",
        "    return text\n",
        "def Sequential(fns):\n",
        "    def new_fn(inputs):\n",
        "      for fn in fns:\n",
        "        inputs = fn(inputs)\n",
        "      return inputs\n",
        "    return new_fn\n",
        "WS = ['نشاط إثرائي اقرأ ثم استنتج','عرف ذلك من النص التالي','السخلة : هي ولد الفتم من الضأن والماعز ساعة وضعه',\n",
        "      'مصر من الفتح الإسلامي حتى قيام الدول المستقلة',\n",
        "      'مصر في عصر الولاة','اقرأ','لتتعرف','التالي','ذلك','النص',\n",
        "      'لاحظ الخريطة التالية لتتعرفها','فكر وناقش']\n",
        "\n",
        "WS2 = ['مرحلة التاسيس و الاستقرار',\n",
        "       'كل من',\n",
        "       'اقرأ الشكل التالي لتتعرف أشهرهم بشيء من التفصيل',\n",
        "       'بشيء من التفصيل',\n",
        "       'حمورابي بالأكدية تلفظ امورابي وتعني المعتلي',\n",
        "       'والآن اقرأ الشكل التالي لتتعرف أدوار الجهاد ضد الصليبيين',\n",
        "       '(بدر الجمالي)','هل تعرف لمـاذا','لاحظ الخريطة الزمنية التالية لتتعرفها',\n",
        "       'ومن خلال','الخريطة الزمنية السابقة'\n",
        "       ,'نستنتج أن',\n",
        "       'رضي الله عنه',\n",
        "       'نعم يا أحبابنا','ق . م',\n",
        "       'التواصل الثقافي والفني مع أفريقيا',\n",
        "       'الصـور المقابلة',\n",
        "       'مـن خـلال',\n",
        "       'الدرس الثاني',\n",
        "       'الدرس الثالث',\n",
        "       'الدرس الرابع',\n",
        "       'الدرس الخامس',\n",
        "       'الدرس',\n",
        "       'ولنبدأ',\n",
        "       'والان تعال',\n",
        "       'والآن تعال معا',\n",
        "       'استثمر المصرى',\n",
        "       'من خلال ما يلي','والآن تعال معنا',\"أولا\",\"رابعا\",\"خامسا\",\"سادسا\",\"سابعا\",\"والآن لاحظ الشكل التالي\"]\n",
        "\n",
        "WORDS=['هل','اقرأ وناقش','معلومة إثرائية','علام يدل','للصف الثاني الثانوي','أ-','ب-','ج-','وهذا ما سوف نتعرفه في الدرس القادم','عزيزي الطالب / عزيزتي الطالبة',\n",
        "         'اقرأ النص التالي','ولعلك تتساءل عزيزي الطالب','يمكنك الإجابة عن هذا التساؤل بعد قراءة النص التالي','وذلك ما سوف نتعرفه في الدرس القادم',\n",
        "  'هل تعرف أسباب فشلها  اقرأ الشكل التالي لتتعرف أهمها','تعرف ذلك من النص التالي',\"لاحظ الخريطة التالية\",\n",
        "          \"وذلك ما سوف نتعرفه في الدرس القادم\",\"والآن تعال معا\",\n",
        "       'الفتوحات الأموية في الغرب الإسلامي',\"ثالثا\",\"ثانيا\",\n",
        "       \"اولا\",\"اقرأ وناقش،المعاني والقيم الواردة\",\n",
        "       \"الفتوحات الإسلامية في عصر الدولة الأموية\",\n",
        "       'لاحظ الخريطة التالية لتتعرف عليها',\n",
        "       \"بعد ان\",\"وبعد ان القينا الضوء علي\",\n",
        "       'وهذا ما سوف نتعرفه',\n",
        "       'الوحدة التالية',\n",
        "       'رواه مسلم','العامل التاريخي',\n",
        "       'لتتعرفها','النص التالي',\"أ.\",\"ب.\",\"ج.\",\"د.\",\"والآن لاحـظ الشك\",\"ثانيا:\",\".. الرسم والنقش:\",\"سادسا:\"]\n",
        "print(len(WORDS))\n",
        "WORDS += WS\n",
        "WORDS += WS2\n",
        "WORDS = list(set(WORDS))\n",
        "text_processor = Sequential([ _remove_diacritics,\n",
        "                             _remove_qoutes,\n",
        "                              _remove_puncs,\n",
        "                              _remove_extra_spaces,\n",
        "                             _remove_repeated_chars ]) #223\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9XYGnBct4s",
        "outputId": "b0196870-94cb-40d0-f2d1-707d5f205df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(WORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZYFHMXDsgQN",
        "outputId": "3563e866-108c-4853-8e19-6c9ef3cf0c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J053eiInjXIs",
        "outputId": "65ce0228-7295-4849-e2e7-0855988ddd19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "preprocessor = ArabertPreprocessor(model_name=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze(model):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    return model"
      ],
      "metadata": {
        "id": "jDqiMtavCsOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGIQ-b2QOlF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8884b03-c533-46b9-aeac-a3860d9aa7f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of parameters is :139221504\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer,AutoModelForSeq2SeqLM, pipeline,AutoModel,AutoTokenizer,BartForConditionalGeneration\n",
        "\n",
        "\n",
        "model_mini= \"asafaya/bert-mini-arabic\"\n",
        "model_med = \"asafaya/bert-medium-arabic\"\n",
        "model_bart = \"moussaKam/AraBART\"\n",
        "model_bart_2 = \"abdalrahmanshahrour/auto-arabic-summarization\"\n",
        "model_name=\"malmarjeh/mbert2mbert-arabic-text-summarization\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_bart_2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_bart_2)\n",
        "\n",
        "#encoder = freeze(AutoModel.from_pretrained(model_bart_2).encoder)\n",
        "print(f\"num of parameters is :{sum([p.numel() for p in model.parameters()])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O3bupYx-OVos",
        "outputId": "d8da186b-53e3-4a9f-87c1-18108384844f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   example_id                                            summary  \\\n",
              "0           0  يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...   \n",
              "1           1  دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...   \n",
              "2           2  أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...   \n",
              "3           3  مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...   \n",
              "4           4  الشعب السوري يصر على استقلال بلدهم من السيطرة ...   \n",
              "\n",
              "                                            document  \n",
              "0  وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...  \n",
              "1  ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...  \n",
              "2  قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...  \n",
              "3  دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...  \n",
              "4  السوريون يصرون على استقلال بلادهم : و مثلما رف...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38640abe-9e90-43c9-b656-26b359ccefb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>summary</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...</td>\n",
              "      <td>وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...</td>\n",
              "      <td>ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...</td>\n",
              "      <td>قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...</td>\n",
              "      <td>دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>الشعب السوري يصر على استقلال بلدهم من السيطرة ...</td>\n",
              "      <td>السوريون يصرون على استقلال بلادهم : و مثلما رف...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38640abe-9e90-43c9-b656-26b359ccefb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38640abe-9e90-43c9-b656-26b359ccefb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38640abe-9e90-43c9-b656-26b359ccefb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import pandas as pd\n",
        "vals = pd.read_json('/content/drive/MyDrive/data/labeled_validation_dataset.jsonl',lines=True)\n",
        "vals['document'] = vals['paragraph']\n",
        "del vals['paragraph']\n",
        "vals.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclEQiLaiRYx",
        "outputId": "76ee1873-5f9a-434e-f990-f41d341ae615"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "778"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "get_len = lambda x: len(tokenizer.tokenize(x))\n",
        "test_data = pd.read_json('/content/drive/MyDrive/data/validation_data.jsonl',lines=True)\n",
        "test_data['document'] = test_data['paragraph']\n",
        "del test_data['paragraph']\n",
        "test_data['document_len'] = test_data['document'].apply(get_len)\n",
        "test_data['document_len'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYqorADSEjkm",
        "outputId": "00f620c0-69f1-4ddf-f589-d60af722b06c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "274"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vals['document_len'] = vals['document'].apply(get_len)\n",
        "vals['summary_len'] = vals['summary'].apply(get_len)\n",
        "vals['document_len'].min()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pd.read_json('predictions.jsonl',lines=True)"
      ],
      "metadata": {
        "id": "TzQ7wXP5GX8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['summary'] = pred['summary']\n",
        "test_data['summary_len'] = test_data['summary'].apply(get_len)\n",
        "test_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ALckL3zlGea3",
        "outputId": "a61d3a47-5483-4021-f466-52689e25b647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   example_id                                           document  \\\n",
              "0           0  وبعد أن ألقينا الضوء على أهم فتوحات بلاد الشام...   \n",
              "1           1  الفتوحات الإسلامية في عصر الخلفاء الراشدين .\\n...   \n",
              "2           2  فتوحات بلاد شمال إفريقيا .\\nقام المسلمون بالتو...   \n",
              "3           3  وبعد أن ألقينا الضوء على أهم فتوحات بلاد شمال ...   \n",
              "4           4  عزيزي الطالب / عزيزتي الطالبة ...\\nبعد أن توقف...   \n",
              "\n",
              "   document_len                                            summary  \\\n",
              "0           460  فتح بيت المقدس 15 ه 636 م لحق عمرو بن العاص با...   \n",
              "1           173  بعد أن انتهى عمر بن الخطاب من خطبته دعاه البطر...   \n",
              "2           125  قام المسلمون بالتوجه غربا لفتح شمال إفريقيا عا...   \n",
              "3           268  ألقينا الضوء على أهم فتوحات بلاد شمال إفريقيا ...   \n",
              "4           116  توقفت حركة الفتوحات الإسلامية بسبب فتنة عثمان ...   \n",
              "\n",
              "   summary_len  \n",
              "0          290  \n",
              "1          124  \n",
              "2           50  \n",
              "3           87  \n",
              "4           44  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8116e77a-6782-4ac9-959b-c2fd40cb0534\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>document</th>\n",
              "      <th>document_len</th>\n",
              "      <th>summary</th>\n",
              "      <th>summary_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>وبعد أن ألقينا الضوء على أهم فتوحات بلاد الشام...</td>\n",
              "      <td>460</td>\n",
              "      <td>فتح بيت المقدس 15 ه 636 م لحق عمرو بن العاص با...</td>\n",
              "      <td>290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>الفتوحات الإسلامية في عصر الخلفاء الراشدين .\\n...</td>\n",
              "      <td>173</td>\n",
              "      <td>بعد أن انتهى عمر بن الخطاب من خطبته دعاه البطر...</td>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>فتوحات بلاد شمال إفريقيا .\\nقام المسلمون بالتو...</td>\n",
              "      <td>125</td>\n",
              "      <td>قام المسلمون بالتوجه غربا لفتح شمال إفريقيا عا...</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>وبعد أن ألقينا الضوء على أهم فتوحات بلاد شمال ...</td>\n",
              "      <td>268</td>\n",
              "      <td>ألقينا الضوء على أهم فتوحات بلاد شمال إفريقيا ...</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>عزيزي الطالب / عزيزتي الطالبة ...\\nبعد أن توقف...</td>\n",
              "      <td>116</td>\n",
              "      <td>توقفت حركة الفتوحات الإسلامية بسبب فتنة عثمان ...</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8116e77a-6782-4ac9-959b-c2fd40cb0534')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8116e77a-6782-4ac9-959b-c2fd40cb0534 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8116e77a-6782-4ac9-959b-c2fd40cb0534');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exts  = pd.read_json('/content/exts.jsonl',lines=True)\n",
        "exts.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "w3hUtg2nJcmi",
        "outputId": "e53f70ab-fe92-40d8-e428-b645cd15625e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            document  \\\n",
              "0  عزيزي الطالب / عزيزتي الطالبة ...\\nبعد أن توقف...   \n",
              "1  : الفتوحات الأموية في المشرق الإسلامي.\\nحرص ول...   \n",
              "2  الفتوحات الأموية في آسيا الصغرى (بلاد الروم).\\...   \n",
              "3  الفتوحات الإسلامية في عصر الدولة الأموية .\\nوو...   \n",
              "4  حسان بن النعمان: والذي انطلق إلى إفريقية في عه...   \n",
              "\n",
              "                                             summary  example_id  \n",
              "0  توقفت حركه الفوتوحات بعدفتنه عثمان حتي قيام ال...         4.0  \n",
              "1  حرص ولاه بني اميه علي استكمال فوتوحات الراشدون...         5.0  \n",
              "2  الفوتوحات الامويه في اسيا الصغري امتدت حتي ارم...         6.0  \n",
              "3    واصل الامويون فتوحاتهم حتي نجحوا في اخضاع اق...         7.0  \n",
              "4    حسان بن النعمان والذي انطلق إلى إفريقية في ع...         8.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e84e710-d1b1-4ec7-9c70-3d44bdf2362e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>summary</th>\n",
              "      <th>example_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>عزيزي الطالب / عزيزتي الطالبة ...\\nبعد أن توقف...</td>\n",
              "      <td>توقفت حركه الفوتوحات بعدفتنه عثمان حتي قيام ال...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>: الفتوحات الأموية في المشرق الإسلامي.\\nحرص ول...</td>\n",
              "      <td>حرص ولاه بني اميه علي استكمال فوتوحات الراشدون...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>الفتوحات الأموية في آسيا الصغرى (بلاد الروم).\\...</td>\n",
              "      <td>الفوتوحات الامويه في اسيا الصغري امتدت حتي ارم...</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>الفتوحات الإسلامية في عصر الدولة الأموية .\\nوو...</td>\n",
              "      <td>واصل الامويون فتوحاتهم حتي نجحوا في اخضاع اق...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>حسان بن النعمان: والذي انطلق إلى إفريقية في عه...</td>\n",
              "      <td>حسان بن النعمان والذي انطلق إلى إفريقية في ع...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e84e710-d1b1-4ec7-9c70-3d44bdf2362e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e84e710-d1b1-4ec7-9c70-3d44bdf2362e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e84e710-d1b1-4ec7-9c70-3d44bdf2362e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exts['document_len'] = exts['document'].apply(get_len)\n",
        "exts['summary_len'] = exts['summary'].apply(get_len)"
      ],
      "metadata": {
        "id": "ZUiijSZYyIgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/data/first_9.csv')\n",
        "data2 = pd.read_csv(\"/content/drive/MyDrive/data/all_data_25_45_words.csv\")\n",
        "del data['address']\n",
        "del data2['Unnamed: 0']\n",
        "data = pd.concat([data,data2]).drop_duplicates().reset_index(drop=True)\n",
        "data['document'] = data['article']\n",
        "del data['article']\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd_eBW9jZM--",
        "outputId": "be74e2ec-e17e-4c99-a646-002dff90573f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209933"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['sum_len'] = data['summary'].apply(lambda x:len(x.split()))\n",
        "data['para_len']= data['document'].apply(lambda x:len(x.split()))"
      ],
      "metadata": {
        "id": "e9s6Y23vxot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['calc'] = data['sum_len']/data['para_len']\n",
        "data = data[(data['calc']<0.40)& (data['calc']>0.3)].reset_index(drop=True).drop_duplicates()\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsCa36lT6PTH",
        "outputId": "bc3bc4e5-8932-4ee7-d3ae-e4788ebde786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63604"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vals['sum_len'] = vals['summary'].apply(lambda x:len(x.split()))\n",
        "vals['para_len']= vals['document'].apply(lambda x:len(x.split()))\n",
        "test_data['sum_len'] = test_data['summary'].apply(lambda x:len(x.split()))\n",
        "test_data['para_len']= test_data['document'].apply(lambda x:len(x.split()))\n",
        "\n",
        "vals['calc'] = vals['sum_len']/vals['para_len']\n",
        "test_data['calc'] = test_data['sum_len']/test_data['para_len']\n",
        "\n",
        "#vals = vals[(vals['calc']<0.41)& (vals['calc']>0.3)].reset_index(drop=True)\n",
        "#test_data = test_data[(test_data['calc']<0.41)& (test_data['calc']>0.3)].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "lBM9ariz08QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data[['document','summary']]\n",
        "vals = vals[['document','summary']]\n",
        "data = data[['document','summary']]\n",
        "data = pd.concat([data,vals]).reset_index(drop=True).drop_duplicates()\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgjtpSTc1hPR",
        "outputId": "8c13bd83-0f40-4d4f-e2dc-b0a1b6c27ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63717"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LCuC5TElaSY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "\n",
        "class SummarizationDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               with_summary=True):\n",
        "      self.data = data\n",
        "      self.with_summary = with_summary\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "  def __getitem__(self,idx):\n",
        "     document = self.data.iloc[idx]['document']\n",
        "    # for word in WORDS :\n",
        "     #   document = document.replace(word,'')\n",
        "     document = text_processor(document)\n",
        "     document = _remove_extra_spaces(document.replace(\"\\n\",\" \"))\n",
        "     if self.with_summary:\n",
        "        summary = self.data.iloc[idx]['summary']\n",
        "        return document,summary\n",
        "     return document\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs,labels = [],[]\n",
        "    with_summary =  len(batch[0])>1\n",
        "    for item in batch:\n",
        "       inputs.append(item[0])\n",
        "       if with_summary:\n",
        "        labels.append(item[1])\n",
        "\n",
        "    inputs = tokenizer(inputs,\n",
        "                       return_tensors='pt',\n",
        "                       padding='longest',\n",
        "                       truncation=True,\n",
        "                       max_length=500)\n",
        "    if with_summary:\n",
        "      labels =  tokenizer(labels,\n",
        "                       return_tensors='pt',\n",
        "                       padding='longest',\n",
        "                       truncation=True,\n",
        "                       max_length=250)\n",
        "      dec_in = {k:v[:,:-1] for k,v in labels.items()}\n",
        "      dec_out = {k:v[:, 1:] for k,v in labels.items()}\n",
        "      dec_in = {'decoder_input_ids':dec_in['input_ids'],\n",
        "                \"decoder_attention_mask\":dec_in['attention_mask']}\n",
        "      inputs.update(dec_in)\n",
        "      return inputs,dec_out\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = SummarizationDataset(data)\n",
        "val_ds = SummarizationDataset(test_data)\n",
        "\n",
        "train_iter = DataLoader(train_ds,batch_size=18,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "val_iter = DataLoader(val_ds,\n",
        "                      batch_size=4,\n",
        "                      num_workers=2,\n",
        "                      pin_memory=True,\n",
        "                      collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "BSS0o3240zCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNxmLoJhIPFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195b0313-2823-4ad9-97c4-f4ed25e97220"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_ids': tensor([[    0,   525,    11,  ..., 39841,    31,     2],\n",
              "         [    0, 16105,    22,  ...,     1,     1,     1],\n",
              "         [    0, 37316,    22,  ...,     1,     1,     1],\n",
              "         [    0,   525,    11,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_input_ids': tensor([[    0,   678,   790,  3935,   379,   333,   243,  6440,    71, 10174,\n",
              "           1797,    67, 10691,    61,  5765,  3881,   578,  4821,   241,  4603,\n",
              "          10016,     6,  3872,     9, 36800,   589, 32329,    45,    19,   131,\n",
              "            320, 15246,  4465, 29144,     5,  2870, 37612,  5136,   552,     4,\n",
              "          12392,  4821,    99, 11752,  5607,  1839,   117,  3815,     5, 13887,\n",
              "             26,   552,  1798,  2267, 29580,   785, 11033,  1011,  5759,     4,\n",
              "          24544,    11,   172, 13887,    26,  7459,   440,    67,  2321,  4439,\n",
              "            407, 10716,     6, 46170, 17686,  5556,   284, 23890,  1797,    67,\n",
              "          10691,   915,  1092,    19,  7459,  2817,  4534,   122,    87,     5,\n",
              "          11910,  4678,   117, 17636, 15350,   159, 37462,    12,  9363,  5623,\n",
              "          27272,    12,  6638,  5572,    30,   283, 16153, 12687,  2640,   272,\n",
              "           3246,    54, 23163,     6,   380,    51,  4648,   418,    37,    38,\n",
              "           2606, 11563,  1129,  6000,   790,  3935,   193,   272,  7396,    87,\n",
              "          26917,  3117,    61,  3563,    33,   202,  2411,    30,    77,   140,\n",
              "          16641,  9068,  1006,    29, 13887,     4, 22102,    39,   440, 22102,\n",
              "             12,    60,    93,  8676,   115,   209,   181,     5,   383,  4082,\n",
              "            927,  1657,    38, 13887, 15960,   427,    59,  7459,   440, 16201,\n",
              "            185,     5, 24841, 35074,  1142, 22988,   225,   790,  3935,  1564,\n",
              "           5537,     4, 19364,    82,   250,    49, 29383, 26410,    39, 14120,\n",
              "          13051,    39,  1081,  3858,    39,  4763,  8917,    17,    10,  5747,\n",
              "          24764, 16564, 14215,    17,   102,    25, 16563, 28783,    39,    79,\n",
              "          39399,     4,     8, 22988,   225, 39841,    68,  3796,   117, 48604,\n",
              "          33242,    11,  2661,    53,   205, 10016,  8212,  7303,  1158,  2482,\n",
              "            579,  1083,  1091,     8,   325, 39954,    80,  1712,    53,    71,\n",
              "          15359,    39,     4,   103,  8018,   579,   522,  5684,  4736],\n",
              "         [    0,    51,    11,  3655,   440,    67,  2321,     6, 42686,  1938,\n",
              "              9,  9068, 42162,  6640,  4103,  5142,  3891, 14703, 34935,   125,\n",
              "            935,   100,   140,   818, 32013,    19,  9068,   122,    87,  2714,\n",
              "           1504,    63,   582,   493,    50,  6003,   582,   440,    27,    59,\n",
              "          26581,    11,  9724,     5,  6640,     5, 10844,  4720,     6,    51,\n",
              "             18, 24747,   269,   267,   440,  6204,  7298,    77, 36177, 19954,\n",
              "            239, 45026,    13,   273,     4,   190, 28704,  7906,    53, 27640,\n",
              "              6, 35670,    39,     4,   983,    77, 36177,  9740, 26451,   440,\n",
              "            100,     6,  6553,  1643,  3343,  6019,    49,    38,  2301,   678,\n",
              "            790,  3935,   193,   678,     9,  1908,    12, 10311,  1229,   422,\n",
              "           9069,    12, 11090,  1506,   440,     6,  2871,  4729, 29580,   785,\n",
              "           8548, 13887,  3813,   570, 46785,    23,   112, 17694,    17,   476,\n",
              "             30,  3935,    68, 30956,     6,     2,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
              "         [    0,   483,  4720, 32294, 28976, 10091,   827,  2870,    95,   319,\n",
              "            333,  5455,  5140,  9599,  1797,    67, 10691,  9567,   678,   131,\n",
              "          17549, 13083,  2144,  2591,     4,  1906,  6468,   120,    37,    67,\n",
              "            968,    67,   476,  2297,   723,  1627,   131,  2023,     6,   678,\n",
              "             17, 10286, 22102,    12,   117,    17,  2655,   495,  1629,  2124,\n",
              "           7989,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
              "         [    0, 27599,    49,  2363,     8,   706, 37316,    22,  3238,   827,\n",
              "           2870, 21691,  1862, 39255,   678, 22566,  4602,    13, 19978,  9784,\n",
              "           6308,  5255,    22,    43,   383,     4, 20314,  4720,  2414,    96,\n",
              "           8956,    41,   825,  3418,   380,  4534,  4342, 34656,  3324,   736,\n",
              "             10, 14445,   399, 29144, 31039, 22566,  4602,    13,   193,     6,\n",
              "            800,   678,    17,    11,  4154,   120,    37,    67,   476,  2297,\n",
              "            723, 21277,   552, 10091,   104,  1673, 35835,    45,   137, 20373,\n",
              "             13,    10, 16331,   589,  4037,     4,  2271, 43953, 16105,    22,\n",
              "            387,   321,   181,     5,  2641,   201, 17917,    13,     2,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
              " {'input_ids': tensor([[  678,   790,  3935,   379,   333,   243,  6440,    71, 10174,  1797,\n",
              "              67, 10691,    61,  5765,  3881,   578,  4821,   241,  4603, 10016,\n",
              "               6,  3872,     9, 36800,   589, 32329,    45,    19,   131,   320,\n",
              "           15246,  4465, 29144,     5,  2870, 37612,  5136,   552,     4, 12392,\n",
              "            4821,    99, 11752,  5607,  1839,   117,  3815,     5, 13887,    26,\n",
              "             552,  1798,  2267, 29580,   785, 11033,  1011,  5759,     4, 24544,\n",
              "              11,   172, 13887,    26,  7459,   440,    67,  2321,  4439,   407,\n",
              "           10716,     6, 46170, 17686,  5556,   284, 23890,  1797,    67, 10691,\n",
              "             915,  1092,    19,  7459,  2817,  4534,   122,    87,     5, 11910,\n",
              "            4678,   117, 17636, 15350,   159, 37462,    12,  9363,  5623, 27272,\n",
              "              12,  6638,  5572,    30,   283, 16153, 12687,  2640,   272,  3246,\n",
              "              54, 23163,     6,   380,    51,  4648,   418,    37,    38,  2606,\n",
              "           11563,  1129,  6000,   790,  3935,   193,   272,  7396,    87, 26917,\n",
              "            3117,    61,  3563,    33,   202,  2411,    30,    77,   140, 16641,\n",
              "            9068,  1006,    29, 13887,     4, 22102,    39,   440, 22102,    12,\n",
              "              60,    93,  8676,   115,   209,   181,     5,   383,  4082,   927,\n",
              "            1657,    38, 13887, 15960,   427,    59,  7459,   440, 16201,   185,\n",
              "               5, 24841, 35074,  1142, 22988,   225,   790,  3935,  1564,  5537,\n",
              "               4, 19364,    82,   250,    49, 29383, 26410,    39, 14120, 13051,\n",
              "              39,  1081,  3858,    39,  4763,  8917,    17,    10,  5747, 24764,\n",
              "           16564, 14215,    17,   102,    25, 16563, 28783,    39,    79, 39399,\n",
              "               4,     8, 22988,   225, 39841,    68,  3796,   117, 48604, 33242,\n",
              "              11,  2661,    53,   205, 10016,  8212,  7303,  1158,  2482,   579,\n",
              "            1083,  1091,     8,   325, 39954,    80,  1712,    53,    71, 15359,\n",
              "              39,     4,   103,  8018,   579,   522,  5684,  4736,     2],\n",
              "          [   51,    11,  3655,   440,    67,  2321,     6, 42686,  1938,     9,\n",
              "            9068, 42162,  6640,  4103,  5142,  3891, 14703, 34935,   125,   935,\n",
              "             100,   140,   818, 32013,    19,  9068,   122,    87,  2714,  1504,\n",
              "              63,   582,   493,    50,  6003,   582,   440,    27,    59, 26581,\n",
              "              11,  9724,     5,  6640,     5, 10844,  4720,     6,    51,    18,\n",
              "           24747,   269,   267,   440,  6204,  7298,    77, 36177, 19954,   239,\n",
              "           45026,    13,   273,     4,   190, 28704,  7906,    53, 27640,     6,\n",
              "           35670,    39,     4,   983,    77, 36177,  9740, 26451,   440,   100,\n",
              "               6,  6553,  1643,  3343,  6019,    49,    38,  2301,   678,   790,\n",
              "            3935,   193,   678,     9,  1908,    12, 10311,  1229,   422,  9069,\n",
              "              12, 11090,  1506,   440,     6,  2871,  4729, 29580,   785,  8548,\n",
              "           13887,  3813,   570, 46785,    23,   112, 17694,    17,   476,    30,\n",
              "            3935,    68, 30956,     6,     2,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
              "          [  483,  4720, 32294, 28976, 10091,   827,  2870,    95,   319,   333,\n",
              "            5455,  5140,  9599,  1797,    67, 10691,  9567,   678,   131, 17549,\n",
              "           13083,  2144,  2591,     4,  1906,  6468,   120,    37,    67,   968,\n",
              "              67,   476,  2297,   723,  1627,   131,  2023,     6,   678,    17,\n",
              "           10286, 22102,    12,   117,    17,  2655,   495,  1629,  2124,  7989,\n",
              "               2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
              "          [27599,    49,  2363,     8,   706, 37316,    22,  3238,   827,  2870,\n",
              "           21691,  1862, 39255,   678, 22566,  4602,    13, 19978,  9784,  6308,\n",
              "            5255,    22,    43,   383,     4, 20314,  4720,  2414,    96,  8956,\n",
              "              41,   825,  3418,   380,  4534,  4342, 34656,  3324,   736,    10,\n",
              "           14445,   399, 29144, 31039, 22566,  4602,    13,   193,     6,   800,\n",
              "             678,    17,    11,  4154,   120,    37,    67,   476,  2297,   723,\n",
              "           21277,   552, 10091,   104,  1673, 35835,    45,   137, 20373,    13,\n",
              "              10, 16331,   589,  4037,     4,  2271, 43953, 16105,    22,   387,\n",
              "             321,   181,     5,  2641,   201, 17917,    13,     2,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "               1,     1,     1,     1,     1,     1,     1,     1,     1]]),\n",
              "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0]])}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "next(iter(val_iter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MBaZcI4f0ZZ"
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R55Pi9ijoxQ2"
      },
      "outputs": [],
      "source": [
        "loss_obj = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "def accuracy(preds,labels,attn_mask):\n",
        "    dim = preds.shape[-1]\n",
        "    pred = preds.view(-1,dim)\n",
        "    label = labels.reshape(-1)\n",
        "    attn= attn_mask.reshape(-1)\n",
        "    pred = pred.argmax(dim=-1)\n",
        "    acc = (pred==label)*attn\n",
        "    return acc.sum()/attn.sum()\n",
        "\n",
        "def greedy_decode_loss(model,context,targets):\n",
        "    bs, seq_len = targets.size()\n",
        "    dec_in = torch.ones([bs,1],dtype=torch.int32).to(targets.device)*tokenizer.cls_token_id\n",
        "    context = model.model.encoder(**context).last_hidden_state\n",
        "    loss = 0\n",
        "    for i in range(1,seq_len):\n",
        "        logits = model.model.decoder(input_ids=dec_in,encoder_hidden_states=context).last_hidden_state\n",
        "        logits = model.lm_head(logits)\n",
        "        tar = targets[:,i]\n",
        "        pred = logits[:,-1,:]\n",
        "        l = loss_obj(pred,tar)\n",
        "        dec_in = torch.concat([dec_in,pred.argmax(dim=-1,keepdim=True)],dim=-1)\n",
        "        loss = loss + l\n",
        "        del logits, pred\n",
        "    loss = loss/(seq_len-1)\n",
        "    hidden_tar = (encoder(input_ids=targets).last_hidden_state).mean(dim=-1)\n",
        "    hidden_pred = (encoder(input_ids=dec_in).last_hidden_state).mean(dim=-1)\n",
        "    sim_loss = - torch.nn.functional.cosine_similarity(hidden_tar,hidden_pred).mean()\n",
        "    return loss, sim_loss\n",
        "\n",
        "\n",
        "def loss_fn(logits,targets):\n",
        "    return loss_obj(logits.reshape(-1,logits.size(-1))\n",
        "                     ,targets.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCAtgzdOoxau"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "\n",
        "mean = lambda x:sum(x)/len(x)\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "def train_epoch(model,train_iter,opt,times=5):\n",
        "    losses,accs = [],[]\n",
        "    model.train()\n",
        "    for i,(enc_in,labels) in enumerate(train_iter):\n",
        "      enc_in = {k:v.to(DEVICE) for k,v in enc_in.items()}\n",
        "      labels = {k:v.to(DEVICE) for k,v in labels.items()}\n",
        "      opt.zero_grad()\n",
        "      with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "        out = model(**enc_in).logits\n",
        "        loss = loss_fn(out,labels['input_ids'])\n",
        "      #scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),\n",
        "      #                                           inputs=model.parameters(),\n",
        "      #                                           create_graph=True)\n",
        "\n",
        "      #inv_scale = 1./scaler.get_scale()\n",
        "      #grad_params = [p * inv_scale for p in scaled_grad_params]\n",
        "\n",
        "        # Computes the penalty term and adds it to the loss\n",
        "      #with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "       #     grad_norm = 0\n",
        "        #    for grad in grad_params:\n",
        "         #       grad_norm += grad.pow(2).sum()\n",
        "          #  grad_norm = grad_norm.sqrt()\n",
        "           # loss = loss + grad_norm\n",
        "\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(opt)\n",
        "      scaler.update()\n",
        "      loss = loss.item()\n",
        "      acc = accuracy(out,\n",
        "                     labels['input_ids'],\n",
        "                     labels['attention_mask']).item()\n",
        "      del out, enc_in, labels\n",
        "      torch.cuda.empty_cache()\n",
        "      if (i+1)%(len(train_iter)//times)==0:\n",
        "        print(f\"Finished Training on {(i+1)*100/len(train_iter):.2f} % of the data, loss:{loss:.3f}, acc:{acc:.3f}.\")\n",
        "      losses.append(loss)\n",
        "      accs.append(acc)\n",
        "    return mean(losses),mean(accs)\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_epoch(model,val_iter):\n",
        "    losses,accs = [],[]\n",
        "    model.eval()\n",
        "    for enc_in,labels in val_iter:\n",
        "        enc_in = {k:v.to(DEVICE) for k,v in enc_in.items()}\n",
        "        labels = {k:v.to(DEVICE) for k,v in labels.items()}\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "             out = model(**enc_in).logits\n",
        "             loss = loss_fn(out,labels['input_ids'])\n",
        "        loss = loss.item()\n",
        "        acc = accuracy(out,\n",
        "                      labels['input_ids'],\n",
        "                      labels['attention_mask']).item()\n",
        "        losses.append(loss)\n",
        "        accs.append(acc)\n",
        "        del out,enc_in,labels\n",
        "        torch.cuda.empty_cache()\n",
        "    return mean(losses),mean(accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrlp26kWZGcw"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class LoraLayer(nn.Module):\n",
        "  def __init__(self,orig_layer,\n",
        "               rank=32,\n",
        "               drop=0.1,\n",
        "               alpha=32.0):\n",
        "      super().__init__()\n",
        "      assert rank > 0\n",
        "      self.scale = alpha/rank\n",
        "      weight_shape = orig_layer.weight.shape\n",
        "\n",
        "      self.drop = nn.Dropout(drop) if drop !=0 else nn.Idenity()\n",
        "      self.lora_A = nn.Parameter(torch.zeros(rank,weight_shape[1])) # (rank, in_shape)\n",
        "      self.lora_B = nn.Parameter(torch.zeros(weight_shape[0],rank)) # (out_shape, rank)\n",
        "      nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "      nn.init.zeros_(self.lora_B)\n",
        "      self.orig_layer = orig_layer\n",
        "      self.merged = False\n",
        "\n",
        "  def reset_parameters(self):\n",
        "      self.orig_layer.reset_parameters()\n",
        "      nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "      nn.init.zeros_(self.lora_B)\n",
        "\n",
        "  def merge(self):\n",
        "      self.orig_layer.weight.data += (self.lora_B @ self.lora_A).transpose(0,1)*self.scale\n",
        "      self.merged=True\n",
        "\n",
        "  def forward(self,x):\n",
        "      out =self.orig_layer(x)\n",
        "      if not self.merged:\n",
        "         out2 = self.drop(x) @ self.lora_A.transpose(0,1)\n",
        "         out2 = (out2  @ self.lora_B.transpose(0,1))*self.scale\n",
        "         return out2+out\n",
        "      return out\n",
        "\n",
        "# freeze the whole model except for lora parameters\n",
        "def freeze_lora(model):\n",
        "    for name,parameter in model.named_parameters():\n",
        "        if \"lora_\" not in name:\n",
        "            parameter.requires_grad=False\n",
        "    return model\n",
        "def merge_lora(ch):\n",
        "    if not ch.merged:\n",
        "       ch.merge()  # merge the weights if its not merged\n",
        "    return ch.orig_layer\n",
        "\n",
        "\n",
        "def get_apply(Module,\n",
        "              instance,\n",
        "              attr_names,\n",
        "              map_fn,**map_fn_kwargs):\n",
        "    for name, ch in Module.named_children():\n",
        "      if isinstance(ch,LoraLayer):\n",
        "           continue\n",
        "      elif isinstance(ch,instance):\n",
        "        for attr_name in attr_names:\n",
        "          attr = map_fn(getattr(ch,attr_name),**map_fn_kwargs)\n",
        "          setattr(ch,attr_name,attr)\n",
        "      else:\n",
        "           get_apply(ch,instance,attr_names,map_fn,**map_fn_kwargs)\n",
        "def convert_lora(module,**kwargs):\n",
        "    return LoraLayer(module,**kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr07C4wl9lSQ"
      },
      "outputs": [],
      "source": [
        "model = model.from_pretrained(\"/content/model_0.79\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "952r4ziHBfS_",
        "outputId": "63a44a72-c39a-4d55-cb36-5f3e926e9bc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'trainable': 139221504, 'untrainable': 0, 'all': 139221504}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def get_params(model):\n",
        "    trainable = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    untrainable = sum([p.numel() for p in model.parameters() if not p.requires_grad])\n",
        "    return {\"trainable\":trainable,\"untrainable\":untrainable,\"all\":(trainable+untrainable)}\n",
        "get_params(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbTNXfJFvnV8"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = model.cuda()\n",
        "opt = torch.optim.AdamW(model.parameters(),lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjLCUK55-DVy",
        "outputId": "3a27ab28-5471-47db-bd85-e1085a348f0d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started Training on epoch:1/2\n",
            "Finished Training on 20.00 % of the data, loss:2.850, acc:0.442.\n",
            "Finished Training on 40.00 % of the data, loss:2.923, acc:0.451.\n",
            "Finished Training on 60.00 % of the data, loss:3.041, acc:0.432.\n",
            "Finished Training on 80.00 % of the data, loss:3.221, acc:0.405.\n",
            "Finished Training on 100.00 % of the data, loss:3.039, acc:0.423.\n",
            "Finished Trainin in 30.92 mins, train_loss:3.033, train_acc:0.430,val_loss:1.456,val_acc:0.767\n",
            ".\n",
            "Started Training on epoch:2/2\n",
            "Finished Training on 20.00 % of the data, loss:2.912, acc:0.444.\n",
            "Finished Training on 40.00 % of the data, loss:2.545, acc:0.506.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "EPOCHS = 2\n",
        "best_val = .355\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  print(f\"Started Training on epoch:{e+1}/{EPOCHS}\")\n",
        "  st = time.time()\n",
        "  train_loss, train_acc = train_epoch(model,train_iter,opt)\n",
        "  val_loss, val_acc = val_epoch(model,val_iter)\n",
        "  if val_acc>best_val:\n",
        "    best_val = val_acc\n",
        "    model.save_pretrained(f'model_{val_acc:.2f}')\n",
        "  print(f\"Finished Trainin in {(time.time()-st)/60:.2f} mins, train_loss:{train_loss:.3f}, train_acc:{train_acc:.3f},val_loss:{val_loss:.3f},val_acc:{val_acc:.3f}\\n.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e6WJji6zzBK"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.from_pretrained(\"/content/model_0.79\").cuda()"
      ],
      "metadata": {
        "id": "6v5TJRTB-HRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAo7h9D5SKRT",
        "outputId": "c1a73d4f-21ce-45d1-909d-43830d7de279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRgQGexrSQy1"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLwumZws_wpb",
        "outputId": "f5672247-4b8f-45b8-8ff1-3898f6065b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1718749951477052"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "def prepare_document(document,tok,max_len=256):\n",
        "    text1 = [d for d  in document.split(\".\")if len(d)>5]\n",
        "    text2 = [d for d in document.split('،') if len(d)>5]\n",
        "    if len(text1) >len(text2):\n",
        "      text = text1\n",
        "      split_on = '.'\n",
        "    else:\n",
        "      text = text2\n",
        "      split_on = '،'\n",
        "    length = 0\n",
        "    count = -1\n",
        "    chunks = []\n",
        "    chunk = ''\n",
        "    if len(text)==1:\n",
        "       text = [ d for d in document.split(\",\") if len(d)>5]\n",
        "       split_on = '،'\n",
        "    else: text\n",
        "    text = re.split(\"والتي|وتم|وكانت|والذي|ومن|وعلي|فقد|أيضا|فتم|فمن|فاصبحت|وهي|و هي|و من|و مع|وهذا|وفي\",document) if (len(text)<2) else text\n",
        "    for sent in text:\n",
        "      com_length = len(sent.split()) +length\n",
        "      count += 1\n",
        "      if com_length< max_len:\n",
        "        chunk = split_on.join([chunk,sent])\n",
        "        length = com_length\n",
        "        if count == len(text)-1:\n",
        "          chunks.append(chunk.strip()[1:])\n",
        "      else:\n",
        "        chunks.append(chunk.strip()[1:])\n",
        "        chunk = ''\n",
        "        chunk = split_on.join([chunk,sent])\n",
        "        length = len(tokenizer.tokenize(sent))\n",
        "    chunks = [c for c in chunks if len(c)>5]\n",
        "    return chunks,split_on\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model,tok,text,**kwargs):\n",
        "    inputs = tok(text,return_tensors='pt',truncation=True)\n",
        "    if kwargs is not None:\n",
        "       num_returns = kwargs.get(\"num_return_sequences\",None)\n",
        "    inputs = {k:v.cuda() for k,v in inputs.items()}\n",
        "    gen = model.generate(**inputs,**kwargs)\n",
        "    gen = gen.cpu().squeeze()\n",
        "    if num_returns is not None:\n",
        "      return tok.batch_decode(gen,skip_spectial_tokens=True)\n",
        "    return tok.decode(gen,skip_special_tokens=True)\n",
        "\n",
        "test_index = 1\n",
        "text = list(vals['document'])[test_index]\n",
        "text = text_processor(text)\n",
        "text = _remove_extra_spaces(text.replace(\"\\n\",\" \"))\n",
        "texts,split = prepare_document(text,tokenizer,max_len=150)\n",
        "print(len(texts),len(text.split()))\n",
        "sums = []\n",
        "for text in texts:\n",
        "    text = preprocessor.preprocess(text)\n",
        "    summary = generate(model,tokenizer,text,max_length=100,\n",
        "             num_beams=10,\n",
        "             no_repeat_ngram_size=2,\n",
        "             repetition_penalty=2.0,\n",
        "             length_penalty=1.0,\n",
        "             )\n",
        "\n",
        "    sums.append(summary)\n",
        "\n",
        "summary = (split+\" \").join([s.replace('.',\"\") for s in sums])\n",
        "rouge.get_scores(summary,list(vals['summary'])[test_index])[0]['rouge-l']['f']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/data/model_.363\")"
      ],
      "metadata": {
        "id": "HsSOpRkp-0hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Was8gPo8PXR3",
        "outputId": "29d1fb4f-45fe-4e70-875a-6e3fcae06df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'كانت منطقة الساحل الشمالي لسوريا ، تابعة للإدارة المركزية للمالك الفنيقية والفينيقيون أنتجوا الأصبغة الملونة بالحضارة الإنسانية ، وتعتبر مهنتهم الأساسية تقوم على التجارة البحرية بين حوض البحر المتوسط وخلال القرن التاسع قبل الميلاد، سقطت المدينة في يد الآشوريين وأخيرا الفرس بدخول الإسكندر المقدوني إلى سوريا في القرن السابع قبل الميلاد وفي عام 476 ق م دخل المدينة منفتخة في الحقيقة على الثقافة اليونانية التي كانت سائدة في الداخل السوري ، ناشرا ثقافات الحضارات عبر التاريخ إلى أن فتحها اللاذقية تحت الحكم البابلي، تستعد مدينة اللاذقية لإحياء ذكرى مولد الإمبراطورية السريانية ، وذلك بعد عامًا من احتفالات بلادها بتعامد الشمس على وجه تمثال الملك رمسيس الثانى بالعاصمة اليونانية أثينا، أعاد بناء اللاذقية الساحلية سلوقس الأول نيكاتور ، مؤسس مدينة آوديكيا اليونانية ، بنائها عام قبل الميلاد ، حسبما ذكر موقع روسيا اليوم، تمر اليوم الذكرى الـ ، على اكتشاف مقبرة فرعونية تحت حكم سلوكس نيكاتور ، وهي مدينة صينية مبنية من القنوات المائية'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEFTikxuqRAf",
        "outputId": "3599bce3-0305-46c3-e914-ee7b44845f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05769230375739672"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "text = list(vals['document'])[test_index]\n",
        "text = preprocessor.preprocess(text)\n",
        "text = text.replace(\"\\n\",\"\")\n",
        "s = generate(model,tokenizer,text,max_length=200,\n",
        "             num_beams=10,\n",
        "             no_repeat_ngram_size=2,\n",
        "             repetition_penalty=2.0,\n",
        "             length_penalty=1.0,\n",
        "             )\n",
        "rouge.get_scores(s,list(vals['summary'])[test_index])[0]['rouge-l']['f']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNBnU2QKHF0q"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_model = freeze(AutoModel.from_pretrained(model_bart_2).encoder.cuda())\n",
        "sim_tok = AutoTokenizer.from_pretrained(model_bart_2)\n",
        "def get_sim(text1,text2):\n",
        "    text1 = {k:v.to(DEVICE) for k,v in sim_tok(text1,return_tensors='pt').items()}\n",
        "    text2 = {k:v.to(DEVICE) for k,v  in sim_tok(text2,return_tensors='pt').items()}\n",
        "    hidden1 = sim_model(**text1).last_hidden_state\n",
        "    hidden2 = sim_model(**text2).last_hidden_state\n",
        "    hidden1 = hidden1.mean(dim=1)\n",
        "    hidden2 = hidden2.mean(dim=1)\n",
        "    sim =torch.nn.functional.cosine_similarity(hidden1,hidden2)\n",
        "    return sim.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI_8uC6Tg-NM",
        "outputId": "39a4a04c-8fa3-4c6d-9ce2-ce6a9fb7ba63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at abdalrahmanshahrour/auto-arabic-summarization were not used when initializing MBartModel: ['final_logits_bias', 'lm_head.weight']\n",
            "- This IS expected if you are initializing MBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(doc):\n",
        "    for word in WORDS :\n",
        "        doc = doc.replace(word,'')\n",
        "    doc = text_processor(doc)\n",
        "    doc = _remove_extra_spaces(doc.replace(\"\\n\",\" \"))\n",
        "    return doc\n",
        "get_sim(exts['summary'].iloc[0],process(exts['document'].iloc[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZc5to94jOBI",
        "outputId": "8bdf05af-9e0e-4b86-89d8-26fb97859b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8266966342926025"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWVq0N5-rOeK"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_json('/content/drive/MyDrive/data/validation_data.jsonl',lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMYvCHTTzw1W",
        "outputId": "3f22caa8-3647-461e-a2fd-ef5f9345bbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "272it [21:18,  4.70s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "gens = []\n",
        "sim_scores =[]\n",
        "rouge_scores = []\n",
        "changed = []\n",
        "lens = []\n",
        "for i,doc in tqdm(enumerate(test_df['paragraph'])):\n",
        "    #for word in WORDS :\n",
        "      #  doc = doc.replace(word,'')\n",
        "    doc = text_processor(doc)\n",
        "    doc = _remove_extra_spaces(doc.replace(\"\\n\",\" \"))\n",
        "    if len(tokenizer.tokenize(doc)) >100:\n",
        "      sumss,scores = [], []\n",
        "      ls = [128, 150, 50, 75]\n",
        "      for l in  ls:\n",
        "        texts, split = prepare_document(doc,tokenizer,max_len=l)\n",
        "        sums = []\n",
        "        for text in texts:\n",
        "          # text = preprocessor.preprocess(text)\n",
        "            summary = generate(model,tokenizer,text,\n",
        "                              max_length=150,\n",
        "                              num_beams=10,\n",
        "                              no_repeat_ngram_size=2,\n",
        "                              repetition_penalty=3.0,\n",
        "                              length_penalty=1.0,)\n",
        "            sums.append(summary)\n",
        "        summary = (split+\" \").join([s.replace('.',\"\") for s in sums])\n",
        "        sumss.append(summary)\n",
        "        scores.append(get_sim(summary,doc))\n",
        "\n",
        "      summary = sumss[np.argmax(scores)]\n",
        "      score = scores[np.argmax(scores)]\n",
        "      best_len = ls[np.argmax(scores)]\n",
        "\n",
        "      lens.append((score,best_len,len(doc.split())))\n",
        "    else:\n",
        "       texts, split = prepare_document(doc,tokenizer,max_len=55)\n",
        "       sums = []\n",
        "       for text in texts:\n",
        "          # text = preprocessor.preprocess(text)\n",
        "          summary = generate(model,tokenizer,text,\n",
        "                            max_length=300,\n",
        "                            num_beams=10,\n",
        "                            no_repeat_ngram_size=3,\n",
        "                            repetition_penalty=3.0,\n",
        "                            length_penalty=1.0,)\n",
        "          sums.append(summary)\n",
        "       summary = (split+\" \").join([s.replace('.',\"\") for s in sums])\n",
        "       score = get_sim(summary,doc)\n",
        "    #score = get_sim(summary,doc)\n",
        "    rouge_scores.append(rouge.get_scores(summary,list(pred['summary'])[i])[0]['rouge-l']['f'])\n",
        "    gt_score = get_sim(pred['summary'][i],doc)\n",
        "    if gt_score > score:\n",
        "       gens.append(pred['summary'][i])\n",
        "       sim_scores.append(gt_score)\n",
        "       changed.append(0)\n",
        "    else:\n",
        "       changed.append(1)\n",
        "       gens.append(summary)\n",
        "       sim_scores.append(score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [f[0] for f in lens]\n",
        "max_lens = [f[1] for f in lens]\n",
        "seq_lens = [f[2] for f in lens]"
      ],
      "metadata": {
        "id": "Uu-TMLNiyWXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(changed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMzhXaGPDJe8",
        "outputId": "5eb136bd-76ff-4baf-de2e-7ffebbad9f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean(sim_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB6OSdRFDZPp",
        "outputId": "9e161b74-2738-4d4a-ef21-a349e5b904b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8912617947687121"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ss = {'scores':scores,\"max_len\":max_lens,\"seq_len\":seq_lens}"
      ],
      "metadata": {
        "id": "xIR157GLY8j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(ss)\n",
        "df.to_csv('max_length.csv')"
      ],
      "metadata": {
        "id": "j-Ab5Vd0Z213"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1UYegQPFaDuX",
        "outputId": "c6c92da9-7eb3-4649-c2e5-644ae32c571b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     scores  max_len  seq_len\n",
              "0  0.979193      150      337\n",
              "1  0.963573       75      124\n",
              "2  0.928714       75       86\n",
              "3  0.904851      100      182\n",
              "4  0.935105       64       84"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1306bcef-3240-4233-9c0f-32806b4af68e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scores</th>\n",
              "      <th>max_len</th>\n",
              "      <th>seq_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.979193</td>\n",
              "      <td>150</td>\n",
              "      <td>337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.963573</td>\n",
              "      <td>75</td>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.928714</td>\n",
              "      <td>75</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.904851</td>\n",
              "      <td>100</td>\n",
              "      <td>182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.935105</td>\n",
              "      <td>64</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1306bcef-3240-4233-9c0f-32806b4af68e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1306bcef-3240-4233-9c0f-32806b4af68e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1306bcef-3240-4233-9c0f-32806b4af68e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sim(process(test_df['paragraph'][0]),pred['summary'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCpmDDjbyi_P",
        "outputId": "c9c4f313-33e3-427c-a8b7-b15b05f0f162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.959555983543396"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7TkLau3_sS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f829bf4-145a-4734-e7f8-268c1334ed7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge: 0.44, cos_sim: 0.89\n"
          ]
        }
      ],
      "source": [
        "print(f\"rouge: {mean(rouge_scores):.2f}, cos_sim: {mean(sim_scores):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fC-czhZPTJkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_k(array,sl):\n",
        "    arr = np.sort(array)[sl]\n",
        "    indices = [np.nonzero(array==i)[0] for i in arr]\n",
        "    return arr, np.concatenate(indices)\n",
        "min_k(np.array(sim_scores),slice(0,10))"
      ],
      "metadata": {
        "id": "LFe5bp2OjPLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d79cfe-df1e-4fa0-f680-c1e7cb4fb12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.5566802 , 0.58024895, 0.62714118, 0.62781996, 0.6409626 ,\n",
              "        0.64835519, 0.64837521, 0.66001654, 0.6612407 , 0.66906011]),\n",
              " array([ 61,  53,  75, 160,  49,  43, 153,  58,  47,  74]))"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = process(test_df['paragraph'].iloc[43])\n",
        "\n",
        "summary =  generate(model,tokenizer,doc,\n",
        "                                  max_length=300,\n",
        "                                  num_beams=8,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  repetition_penalty=3.0,\n",
        "                                  length_penalty=1.0)\n",
        "get_sim(summary,doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUlOen3USKo",
        "outputId": "d0199dc3-57c4-4395-ead9-b94fb8edfbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5171349048614502"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zOjtXo4xBwpU",
        "outputId": "dcd71aa0-ed37-4a8c-dd7e-7bf5f2015d4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'كانت الصناعة أقل نسبيا من الزراعة من حيث أهميتها، أنشأت روما مصانع صغيرة من أجل صناعة الفخار بجانب الصناعات اليدوية، كما تم استخراج كميات كبيرة من الذهب والفضة والتي تستخدم في صناعة العملة والمجوهرات، وفرت المحاجر الحجارة لبناء المشروعات المختلفة'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "pred['summary'][53]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdXqDNZodZiO",
        "outputId": "2ee2ffcd-6005-4a9d-800d-30fc86121e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9007388195570778"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.array(gens)[np.array(changed,dtype=bool)].tolist()[-1]"
      ],
      "metadata": {
        "id": "u3ETj-dFioJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH7sJIbhWV0J",
        "outputId": "d0e4590d-32dd-4697-ca4b-e9a909ae333e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df['paragraph'][271].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDhjwI8UWcJq",
        "outputId": "6144dff8-0cab-4a4a-89e8-81a37ada6901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "383"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "252/383"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuKXnpRGWlUz",
        "outputId": "580fcfed-62cc-4fcb-ea86-793b7754c70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6579634464751958"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =model.cuda()"
      ],
      "metadata": {
        "id": "FMU-uv9QagdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scss = []\n",
        "for i,idx in tqdm(enumerate(range(len(test_df)))):\n",
        "    doc = test_df['paragraph'][idx]\n",
        "    #for word in WORDS :\n",
        "     #       doc = doc.replace(word,'')\n",
        "    doc = text_processor(doc)\n",
        "    doc = _remove_extra_spaces(doc.replace(\"\\n\",\" \"))\n",
        "    summary =  generate(model,tokenizer,doc,\n",
        "                                  max_length=300,\n",
        "                                  num_beams=8,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  repetition_penalty=3.0,\n",
        "                                  length_penalty=1.0,do_sample=True)\n",
        "    sc = get_sim(summary,doc)\n",
        "    scss.append((sim_scores[i],sc))\n",
        "    if sc > sim_scores[i]:\n",
        "      sim_scores[i] = sc\n",
        "      gens[i] = summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvxUNRmzk5M",
        "outputId": "25fe9a51-3b28-4bc4-d03f-bb757d8d2f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "272it [05:53,  1.30s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [old<new for old,new in scss]\n",
        "np.array(sim_scores)[indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JjcjBhjW51R",
        "outputId": "c2c2e812-ec9d-4680-c69c-ae82b6570ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9033426 , 0.86349201, 0.79642832, 0.87678075])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sim_scores).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtEDsVzOaHIv",
        "outputId": "2740f051-9f15-40e0-c596-c8534e8ad324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9060768822536749"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = test_data['document'][66]\n",
        "for word in WORDS :\n",
        "        text=text.replace(word,' ')\n",
        "text = text_processor(text).replace(\"\\n\", \"\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "T2w0Ka4aULo8",
        "outputId": "c45bb835-45df-482a-a330-8bd2652d82b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'الحياة الفنيةظلت الفنـون مزدهرة فـي مصـر، ويشهد بذلـك مـا عثـر عليـه فـي مـدينـة الإسكندرية ولا يزال معروضا حتى الآن بالمتحف اليوناني الروماني ويمكن تقسيم الفـن حينذاك إلى قسمين رئيسين فن الطبقة الحاكمة ويمثله تماثيل الأباطرة الرومان والنقوش التي زينت بـهـا جـدران التوابيت الحجريةالفن الشعبي الذي يتمثل في آلاف التماثيل الصغيرة والأواني الفخارية و الحجرية والمعدنية التي تزخر بـهـا المتاحف، ك كثر استخدام الفسيفساء في الأرضيات بوجـه خـاص، تمثـل مناظرها أشخاص وحيوانات منهـا الحقيقيـة ومنها الخياليةالفن القبطى وفي ظل المسيحية نشأ فـن يعبر عن الدين الجديـد فـي إطـار المـوروث العميـق مـن الفـن المصرى والشرقي وقـد عـرف هذا الفن بالفن القبطي، وتمثله في الكنائس والأديرة القديمة والتي لاتزال آثارها باقية حـتـى الـيـوم وتأثر الفنان القبطى بخصائص الفـن المصرى القديـم فـي التصوير والرسم التشكيلي والنحت والعمارة ومن أمثلة الآثار القبطية في مصر مايليالكنيسة المعلقة وتقع في حي مصر القديمة بمحافظة القاهرة'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MrdCAGacOspo",
        "outputId": "1bdaf101-9e0b-4067-b438-a7ce8bf1a671"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     example_id                                          paragraph\n",
              "0             0  وبعد أن ألقينا الضوء على أهم فتوحات بلاد الشام...\n",
              "1             1  الفتوحات الإسلامية في عصر الخلفاء الراشدين .\\n...\n",
              "2             2  فتوحات بلاد شمال إفريقيا .\\nقام المسلمون بالتو...\n",
              "3             3  وبعد أن ألقينا الضوء على أهم فتوحات بلاد شمال ...\n",
              "4             4  عزيزي الطالب / عزيزتي الطالبة ...\\nبعد أن توقف...\n",
              "..          ...                                                ...\n",
              "267         267  وظهرت كثير من الدراسات قصرت اهتمامها على الأدو...\n",
              "268         268  كانت الزراعة المصرية في هذه الحقبة تمر بعصرها ...\n",
              "269         269  حشدت فرنسا قواتها وأعلنت الحرب في 19 يوليو أي ...\n",
              "270         270  تعتبر قناة السويس هي من أهم الطرق الملاحية الت...\n",
              "271         271  فالثورة الفرنسية ، وهي محل شاهد في صراع الطبقا...\n",
              "\n",
              "[272 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf382672-6f10-4bfe-8532-6e3f4281c719\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>paragraph</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>وبعد أن ألقينا الضوء على أهم فتوحات بلاد الشام...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>الفتوحات الإسلامية في عصر الخلفاء الراشدين .\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>فتوحات بلاد شمال إفريقيا .\\nقام المسلمون بالتو...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>وبعد أن ألقينا الضوء على أهم فتوحات بلاد شمال ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>عزيزي الطالب / عزيزتي الطالبة ...\\nبعد أن توقف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>267</td>\n",
              "      <td>وظهرت كثير من الدراسات قصرت اهتمامها على الأدو...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>268</td>\n",
              "      <td>كانت الزراعة المصرية في هذه الحقبة تمر بعصرها ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>269</td>\n",
              "      <td>حشدت فرنسا قواتها وأعلنت الحرب في 19 يوليو أي ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>270</td>\n",
              "      <td>تعتبر قناة السويس هي من أهم الطرق الملاحية الت...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>271</td>\n",
              "      <td>فالثورة الفرنسية ، وهي محل شاهد في صراع الطبقا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>272 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf382672-6f10-4bfe-8532-6e3f4281c719')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf382672-6f10-4bfe-8532-6e3f4281c719 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf382672-6f10-4bfe-8532-6e3f4281c719');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "test_df = pd.read_json('/content/drive/MyDrive/data/validation_data.jsonl',lines=True)\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjtLp-ZPzLNQ"
      },
      "outputs": [],
      "source": [
        "test_df['summary'] = gens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIAMOfrR1Rq2"
      },
      "outputs": [],
      "source": [
        "test_df.drop(['paragraph'],axis=1,inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBZPBaR1Yry"
      },
      "outputs": [],
      "source": [
        "test_df.to_json(\"predictions15.jsonl\", lines=True, orient='records', force_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/data/model\")"
      ],
      "metadata": {
        "id": "CrH2iew6WsF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6-eeasKWy9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}